{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI7JTat7vmqQ"
      },
      "outputs": [],
      "source": [
        "# INCEPTION V3 NEURAL NETWORK TO CLASSIFY IMAGES AS HAVING POTHOLES OR NOT \n",
        "\n",
        "# Sources cited:\n",
        "# https://keras.io/guides/transfer_learning/\n",
        "# https://keras.io/api/applications/#usage-examples-for-image-classification-models\n",
        "# https://keras.io/api/data_loading/\n",
        "# https://medium.com/@italojs/saving-your-weights-for-each-epoch-keras-callbacks-b494d9648202\n",
        "# https://datascience.stackexchange.com/questions/104572/does-validation-split-in-tf-keras-preprocessing-image-dataset-from-directory-res\n",
        "\n",
        "# importing files and libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "import keras\n",
        "from PIL import Image\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score , recall_score, f1_score\n",
        "from sklearn.metrics import top_k_accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReMkTc0mUL-T",
        "outputId": "227fb70b-3644-4787-e51a-d538ec1fe6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google drive\n",
        "# The dataset is stored on the drive\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive',force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KiaehtJJvld"
      },
      "outputs": [],
      "source": [
        "filepath = '/content/drive/MyDrive/Senior_Design/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8aDtwrEY3fE"
      },
      "outputs": [],
      "source": [
        "# Configuration environment\n",
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"balajisath\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"75272a5becccf25ffbceee62d646c0a9\" # key from the json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NN9zvBAcU_r",
        "outputId": "0e4d4633-943c-4cd7-8e60-cc42ca762ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Senior_Design/Data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Senior_Design/Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJQ8UeD7ZazW",
        "outputId": "5c372750-bdde-401b-bbdc-188a8c9ca583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading nienaber-potholes-2-complex.zip to /content/drive/MyDrive/Senior_Design/Data\n",
            "100% 32.7G/32.7G [18:09<00:00, 31.8MB/s]\n",
            "100% 32.7G/32.7G [18:09<00:00, 32.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "#!kaggle datasets download -d felipemuller5/nienaber-potholes-2-complex --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeM31D1BQL3_",
        "outputId": "d7c01cf6-5992-424f-b56b-9589a096de11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading nienaber-potholes-1-simplex.zip to /content/drive/MyDrive/Senior_Design/Data\n",
            "100% 21.6G/21.6G [12:28<00:00, 41.3MB/s]\n",
            "100% 21.6G/21.6G [12:28<00:00, 31.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "#!kaggle datasets download -d felipemuller5/nienaber-potholes-1-simplex --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmxymENON_N1"
      },
      "outputs": [],
      "source": [
        "#folder='/content/drive/MyDrive/CS523_Deep_Learning/Data/train/c'\n",
        "\n",
        "#filelist = glob.glob(folder + '0' + '/*.jpg')\n",
        "#train_images = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
        "#train_labels = np.zeros((train_images.shape[0],), dtype=int)\n",
        "\n",
        "#for i in range(1,10):\n",
        "#    folder_i = folder+str(i)\n",
        "#    filelist = glob.glob(folder_i + '/*.jpg')\n",
        "#    temp=np.array([np.array(Image.open(fname)) for fname in filelist])\n",
        "#    train_images = np.vstack((train_images, temp))\n",
        "#    train_labels = np.hstack((train_labels, np.zeros((temp.shape[0],), dtype=int) + i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b549A--aVzi_"
      },
      "outputs": [],
      "source": [
        "#folder='/content/drive/MyDrive/CS523_Deep_Learning/Data/TestData/c'\n",
        "\n",
        "#filelist = glob.glob(folder + '0' + '/*.jpg')\n",
        "#test_images = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
        "#test_labels = np.zeros((test_images.shape[0],), dtype=int)\n",
        "\n",
        "#for i in range(1,10):\n",
        "#    folder_i = folder+str(i)\n",
        "#    filelist = glob.glob(folder_i + '/*.jpg')\n",
        "#    temp=np.array([np.array(Image.open(fname)) for fname in filelist])\n",
        "#    test_images = np.vstack((test_images, temp))\n",
        "#    test_labels = np.hstack((test_labels, np.zeros((temp.shape[0],), dtype=int) + i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "467bNG5nteeK",
        "outputId": "d52ec205-3713-4a40-98d9-9f7c17fc9bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7110 files belonging to 2 classes.\n",
            "Using 6399 files for training.\n",
            "Found 4736 files belonging to 2 classes.\n",
            "Using 4263 files for training.\n",
            "Found 7110 files belonging to 2 classes.\n",
            "Using 711 files for validation.\n",
            "Found 4736 files belonging to 2 classes.\n",
            "Using 473 files for validation.\n",
            "Found 984 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Extracting training, validation and test data sets\n",
        "\n",
        "train_ds1 = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=filepath+'Data/Train1',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    validation_split=0.1,\n",
        "    subset='training',\n",
        "    image_size=(480, 640))\n",
        "\n",
        "train_ds2 = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=filepath+'Data/Train2',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    validation_split=0.1,\n",
        "    subset='training',\n",
        "    image_size=(480, 640))\n",
        "\n",
        "train_ds=train_ds1.concatenate(train_ds2)\n",
        "\n",
        "val_ds1 = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=filepath+'Data/Train1',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    validation_split=0.1,\n",
        "    subset='validation',\n",
        "    image_size=(480, 640))\n",
        "\n",
        "val_ds2 = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=filepath+'Data/Train2',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    validation_split=0.1,\n",
        "    subset='validation',\n",
        "    image_size=(480, 640))\n",
        "\n",
        "val_ds=val_ds1.concatenate(val_ds2)\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=filepath+'Data/Test1',\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    batch_size=32,\n",
        "    seed=42,\n",
        "    shuffle=True,\n",
        "    image_size=(480, 640))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX47mRF7bE2X",
        "outputId": "d9e115e6-7ef0-4a54-e09e-6cbd5f0b6eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 480, 640, 3)]     0         \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 13, 18, 2048)      21802784  \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 2050      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,903,010\n",
            "Trainable params: 2,100,226\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 121s 325ms/step - loss: 3.2285 - accuracy: 0.7049 - val_loss: 0.6058 - val_accuracy: 0.7804\n",
            "Epoch 2/10\n",
            "334/334 [==============================] - 107s 313ms/step - loss: 0.5459 - accuracy: 0.7677 - val_loss: 0.5446 - val_accuracy: 0.7804\n",
            "Epoch 3/10\n",
            "334/334 [==============================] - 107s 313ms/step - loss: 0.5272 - accuracy: 0.7754 - val_loss: 0.5262 - val_accuracy: 0.7804\n",
            "Epoch 4/10\n",
            "334/334 [==============================] - 107s 313ms/step - loss: 0.5182 - accuracy: 0.7765 - val_loss: 0.4711 - val_accuracy: 0.7804\n",
            "Epoch 5/10\n",
            "334/334 [==============================] - 106s 310ms/step - loss: 0.5052 - accuracy: 0.7832 - val_loss: 0.5771 - val_accuracy: 0.7812\n",
            "Epoch 6/10\n",
            "334/334 [==============================] - 108s 316ms/step - loss: 0.5141 - accuracy: 0.7784 - val_loss: 0.4633 - val_accuracy: 0.7804\n",
            "Epoch 7/10\n",
            "334/334 [==============================] - 107s 314ms/step - loss: 0.5031 - accuracy: 0.7765 - val_loss: 0.5175 - val_accuracy: 0.7804\n",
            "Epoch 8/10\n",
            "334/334 [==============================] - 108s 315ms/step - loss: 0.5093 - accuracy: 0.7768 - val_loss: 0.4716 - val_accuracy: 0.7804\n",
            "Epoch 9/10\n",
            "334/334 [==============================] - 107s 313ms/step - loss: 0.5049 - accuracy: 0.7762 - val_loss: 0.4844 - val_accuracy: 0.7804\n",
            "Epoch 10/10\n",
            "334/334 [==============================] - 107s 315ms/step - loss: 0.4961 - accuracy: 0.7775 - val_loss: 0.4788 - val_accuracy: 0.7804\n"
          ]
        }
      ],
      "source": [
        "# Inception V3 model with transfer learning\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(480, 640, 3))\n",
        "\n",
        "base_model.trainable = False\n",
        "inputs = keras.Input(shape=(480, 640, 3))\n",
        "\n",
        "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "inputs = scale_layer(inputs)\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.4)(x) \n",
        "\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = keras.layers.Dropout(0.4)(x) \n",
        "\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history=model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeMJIG2bdhi3",
        "outputId": "0559df28-1ac6-4c89-f3bc-58d0bf9e2a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_12 (InputLayer)       [(None, 480, 640, 3)]     0         \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 13, 18, 2048)      21802784  \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 2050      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,903,010\n",
            "Trainable params: 23,868,578\n",
            "Non-trainable params: 34,432\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.7788\n",
            "Epoch 1: loss improved from inf to 0.49302, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 173s 479ms/step - loss: 0.4930 - accuracy: 0.7788 - val_loss: 0.4287 - val_accuracy: 0.7922\n",
            "Epoch 2/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.8274\n",
            "Epoch 2: loss improved from 0.49302 to 0.41331, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 162s 478ms/step - loss: 0.4133 - accuracy: 0.8274 - val_loss: 0.4234 - val_accuracy: 0.8480\n",
            "Epoch 3/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.8582\n",
            "Epoch 3: loss improved from 0.41331 to 0.33818, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 162s 478ms/step - loss: 0.3382 - accuracy: 0.8582 - val_loss: 0.3976 - val_accuracy: 0.8133\n",
            "Epoch 4/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.3059 - accuracy: 0.8739\n",
            "Epoch 4: loss improved from 0.33818 to 0.30588, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 164s 483ms/step - loss: 0.3059 - accuracy: 0.8739 - val_loss: 0.2721 - val_accuracy: 0.8860\n",
            "Epoch 5/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.2527 - accuracy: 0.8907\n",
            "Epoch 5: loss improved from 0.30588 to 0.25274, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 159s 470ms/step - loss: 0.2527 - accuracy: 0.8907 - val_loss: 0.3083 - val_accuracy: 0.8716\n",
            "Epoch 6/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9055\n",
            "Epoch 6: loss improved from 0.25274 to 0.21878, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 161s 474ms/step - loss: 0.2188 - accuracy: 0.9055 - val_loss: 0.2310 - val_accuracy: 0.9003\n",
            "Epoch 7/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.1866 - accuracy: 0.9217\n",
            "Epoch 7: loss improved from 0.21878 to 0.18661, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 161s 477ms/step - loss: 0.1866 - accuracy: 0.9217 - val_loss: 0.3657 - val_accuracy: 0.8159\n",
            "Epoch 8/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9332\n",
            "Epoch 8: loss improved from 0.18661 to 0.15703, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 160s 473ms/step - loss: 0.1570 - accuracy: 0.9332 - val_loss: 0.2040 - val_accuracy: 0.9248\n",
            "Epoch 9/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9444\n",
            "Epoch 9: loss improved from 0.15703 to 0.13896, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 161s 475ms/step - loss: 0.1390 - accuracy: 0.9444 - val_loss: 0.1897 - val_accuracy: 0.9358\n",
            "Epoch 10/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.1109 - accuracy: 0.9574\n",
            "Epoch 10: loss improved from 0.13896 to 0.11093, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 162s 479ms/step - loss: 0.1109 - accuracy: 0.9574 - val_loss: 0.2112 - val_accuracy: 0.9231\n",
            "Epoch 11/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9675\n",
            "Epoch 11: loss improved from 0.11093 to 0.08742, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 160s 473ms/step - loss: 0.0874 - accuracy: 0.9675 - val_loss: 0.2502 - val_accuracy: 0.9299\n",
            "Epoch 12/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9646\n",
            "Epoch 12: loss did not improve from 0.08742\n",
            "334/334 [==============================] - 116s 340ms/step - loss: 0.0968 - accuracy: 0.9646 - val_loss: 0.1790 - val_accuracy: 0.9392\n",
            "Epoch 13/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9741\n",
            "Epoch 13: loss improved from 0.08742 to 0.07130, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 162s 477ms/step - loss: 0.0713 - accuracy: 0.9741 - val_loss: 0.1738 - val_accuracy: 0.9485\n",
            "Epoch 14/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9800\n",
            "Epoch 14: loss improved from 0.07130 to 0.05625, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334/334 [==============================] - 159s 470ms/step - loss: 0.0562 - accuracy: 0.9800 - val_loss: 0.1800 - val_accuracy: 0.9519\n",
            "Epoch 15/15\n",
            "334/334 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9820\n",
            "Epoch 15: loss improved from 0.05625 to 0.05200, saving model to /content/drive/MyDrive/Senior_Design/Models/InceptionV3_Sample_Net\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r334/334 [==============================] - 164s 483ms/step - loss: 0.0520 - accuracy: 0.9820 - val_loss: 0.1521 - val_accuracy: 0.9620\n"
          ]
        }
      ],
      "source": [
        "# Inception V3 model with fine tuning\n",
        "\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath+'Models/InceptionV3_Sample_Net', monitor='loss', verbose=1, save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "history=model.fit(train_ds, epochs=15, validation_data=val_ds, callbacks=[checkpoint])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "8ydMIYCRa5Hw",
        "outputId": "1116db69-ae6b-4a60-ff88-a3421b9faa75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efc7486db20>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plots\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veJL8r89LNu1"
      },
      "outputs": [],
      "source": [
        "model=keras.models.load_model(filepath+'Models/InceptionV3_Sample_Net')\n",
        "# Use filepath+'Models/InceptionV3_Sample_Net' if you want to access a sample net already trained by us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s75qF-No8fpQ",
        "outputId": "2fb769dd-4fbf-4b05-dfeb-4a91bde38a82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 46s 1s/step - loss: 0.4057 - accuracy: 0.9136\n",
            "0.9136179089546204\n"
          ]
        }
      ],
      "source": [
        "# Test Accuracy\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqzL8CqqucZ8",
        "outputId": "4b08d920-6f9a-4b50-e1e6-6cc8fe891374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31/31 [==============================] - 49s 2s/step\n"
          ]
        }
      ],
      "source": [
        "# y_pred and y_true for calculating metrics\n",
        "\n",
        "y_pred_t=model.predict(test_ds)\n",
        "y_pred=np.argmax(y_pred_t, axis=1)\n",
        "\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgrcXVpHkwzg"
      },
      "outputs": [],
      "source": [
        "# Top 2 accuracy\n",
        "\n",
        "#top_k_accuracy_score(y_true, y_pred_t, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sn-MsemB86Wh"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "#cm=confusion_matrix(y_true, y_pred)\n",
        "#print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rtsdDoLuOWUE"
      },
      "outputs": [],
      "source": [
        "# Precision\n",
        "\n",
        "#print(precision_score(y_true,y_pred,average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5ElKRuiCZ_0a"
      },
      "outputs": [],
      "source": [
        "# Recall\n",
        "\n",
        "#print(recall_score(y_true,y_pred,average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fmCjmujcaHi4"
      },
      "outputs": [],
      "source": [
        "# F1 Score\n",
        "\n",
        "#print(f1_score(y_true,y_pred,average='micro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QKyQW05Qcr0c"
      },
      "outputs": [],
      "source": [
        "# Classification Report \n",
        "\n",
        "#print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RlSfUyIjZsUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}